import abc
import typing as t

import jax
import jax.numpy as jnp
from flax import linen as nn

from flax.core.scope import VariableDict

Embedding = jax.Array  # of size `embed_dim`

Observation = t.TypeVar("Observation")
Context = t.TypeVar("Context")
Prediction = t.TypeVar("Prediction")


class Codec(nn.Module, abc.ABC):
    """The abstract interface of a codec."""

    embed_dim: int

    def setup(self) -> None:
        """"""
        pass

    @abc.abstractmethod
    def encode(self, x: Observation) -> t.Tuple[Embedding, Context]:
        """Encode an observation.

        :param x: An observation of the data type expected by the codec.
        :return: A pair containing

             - an embedding vector of shape ``(self.embed_dim,)``
             - a context containing the embeddings for the substructures of ``x``."""
        ...

    @abc.abstractmethod
    def decode(self, conditioning_vector: Embedding, context: Context) -> Prediction:
        """Turn a ``conditioning_vector`` into a predicted probability distribution,
        using the embeddings in the ``context`` in places where autoregressive sampling would occur.

        :param conditioning_vector: Conditioning vector of shape ``(self.embed_dim,)``.
        :param context: Embeddings of the substructures as given by ``encode``.
        :return: A representation of the probability distribution predicted from the conditioning vector.
        """
        ...

    @abc.abstractmethod
    def sample(self, conditioning_vector: Embedding) -> t.Tuple[Observation, Embedding]:
        """
        Sample a single observation, as conditioned by the ``conditioning_vector``.

        :param conditioning_vector: Conditioning vector of shape ``(self.embed_dim,)``.
        :return: A sample from the probability predicted by the conditioning vector, and its embedding.
        """
        ...

    @abc.abstractmethod
    def loss(
        self, x: Observation, prediction: Prediction
    ) -> jnp.ndarray:  # of shape ()
        """Returns the negative log-likelyhood of ``x`` in the provided distribution.

        :param x: An observation of the data type expected by the codec.
        :param prediction: A representation of a probability distribution.
        :return: The negative log-likelyhood of ``x`` in this distribution."""
        ...

    @abc.abstractmethod
    def example(self) -> Observation:
        """Convenience function which provides an example input for the model.

        :return: An example observation of the data type expected by the codec."""
        ...


class MetaLearner(nn.Module):
    """The ``codec`` is embedded in a ``Metalearner`` which holds an additionnal ``starting_embedding`` parameter.
    The ``starting_embedding`` is the conditioning vector from which the autoregressive sampling starts.

    This module works with single observations (not batches). In practice, the model is vmapped over in the following ``BatchMetaLearner``.
    """

    codec_in: Codec

    def setup(self):
        """:meta private:"""
        self.codec = self.codec_in.clone()

        self.starting_embedding = self.param(
            "starting_embedding", nn.zeros, (self.codec.embed_dim,)
        )

    def __call__(self, x: Observation):
        """Compute the negative log-likelyhood of sampling ``x`` from the model.

        We want to find the likelyhood of sampling ``x`` starting from the ``starting_embedding``.
        Sampling is autoregressive for complex structures, so the probability of ``x`` is the product of the probabilities of
        sampling each ``x_i`` conditioned on having sampled ``x_1, ..., x_(i-1)``. Therefore we need the intermediate embeddings
        ``embed(x1, ..., x_(i-1))`` to compute the probability of sampling each ``x_i``.

        :param x: An observation.
        :return: The negative log-likelyhood of ``x`` in the distribution generated by the model.
        """

        # Hence the computation is done in two steps:
        # 1/ compute the embeddings of the substructures of x. They are returned in the `context` variable:
        embedding, context = self.codec.encode(x=x)

        # 2/ predict the next column starting from the `starting_embedding` and as-if `x` was autoregressively sampled:
        prediction = self.codec.decode(
            conditioning_vector=self.starting_embedding, context=context
        )

        # then we can evaluate P(x) = pred1(x1) x pred2(x2) x ...
        loss_x = self.codec.loss(x=x, prediction=prediction)
        return loss_x

    def sample(self):
        """:return: A sample from the distribution generated by the model."""
        sample = self.codec.sample(conditioning_vector=self.starting_embedding)
        return sample

    def example(self):
        """:return: An example observation of the data type expected by the model."""
        return self.codec_in.example()


class BatchMetaLearner:
    """The ``MetaLearner`` is vmapped over to be able to train and sample from the model."""

    def __init__(self, codec_in: Codec):
        """:param codec_in: The codec corresponding to the data type generated by the model."""
        # `metalearner` is a stateful flax module, we convert all the methods we need to pure jax functions:
        metalearner = MetaLearner(codec_in=codec_in, parent=None)

        self.init_fun = lambda rng, x: metalearner.init(
            rngs={"params": rng},
            x=x,
        )

        self.apply_fun = lambda params, x: metalearner.apply(
            variables={"params": params},
            x=x,
        )

        self.sample_fun = lambda params, rng: metalearner.apply(
            variables={"params": params},
            rngs={"sample": rng},
            method="sample",
        )

        self.example = metalearner.example()

    def init(self, rng):
        """Initalize the parameters of the model."""
        # x is a single observation (and not a batch), the example provided by the Codecs can be used
        params = self.init_fun(rng=rng, x=self.example)["params"]
        return params

    def loss_and_per_example_grad(self, params: VariableDict, xs: Observation):
        """For a batch of observations ``xs``, compute the average loss and *per-instance* gradients
        (per-instance gradients are required for DP-SGD, so that they can be clipped).
        """
        # grad then vmap to obtain instance-level gradients
        grad_apply_fun = jax.value_and_grad(fun=self.apply_fun)
        vmapped_grad_apply_fun = jax.vmap(grad_apply_fun, in_axes=(None, 0))

        per_ex_loss, per_ex_grad = vmapped_grad_apply_fun(params, xs)
        return per_ex_loss.mean(), per_ex_grad

    def loss_and_grad(self, params: VariableDict, xs: Observation):
        """For a batch of observations ``xs``, compute the average loss and its associated gradient."""
        # vmap first, average the losses, then grad to obtain batch-level gradient
        vmapped_apply_fun = jax.vmap(self.apply_fun, in_axes=(None, 0))
        scalar_apply_fun = lambda params, xs: vmapped_apply_fun(params, xs).mean()
        grad_vmapped_apply_fun = jax.value_and_grad(fun=scalar_apply_fun)

        batch_loss, batch_grad = grad_vmapped_apply_fun(params, xs)
        return batch_loss, batch_grad

    def sample(self, params: VariableDict, rng, size: int):
        """Sample a batch of chosen size from the model.

        :param params:
        :param rng:
        :param size:
        :return: A batch of chosen size from the model."""
        vmapped_sample_fun = jax.vmap(self.sample_fun, in_axes=(None, 0))

        rngs = jax.random.split(rng, size)
        samples, embeddings = vmapped_sample_fun(params, rngs)
        return samples
