"""A genererative model for complex and hierarchical data"""
from __future__ import annotations

from typing import Tuple, Any, Dict, Callable

import flax.traverse_util
import jax
import jax.numpy as jnp
from flax import linen as nn

from gallimimus.codec.abstract_codec import Codec, Observation

from flax.core.scope import VariableDict
from jax.random import PRNGKeyArray


ModelDict = Dict[str, Codec]
ParamDict = Dict[str, VariableDict]


def init_shared_trained_param_dict(rng, model_dict, params_dict):
    trained_params_dict = {}

    for path, model in model_dict.items():
        if path not in params_dict:
            rng, rng2 = jax.random.split(rng, 2)
            init_params = model.init(rngs=rng, method=model.init_pass)["params"]

            trained_params_dict[path] = init_params

    return flax.core.frozen_dict.freeze(trained_params_dict)


class UnitMetaLearner(nn.Module):
    """The ``codec`` is embedded in a ``Metalearner`` which holds an additionnal ``starting_embedding`` parameter.
    The ``starting_embedding`` is the conditioning vector from which the autoregressive sampling starts.

    This module works with single observations (not batches). In practice, the model is vmapped over in the following ``BatchMetaLearner``.
    """

    codec_in: Codec
    model_dict: ModelDict
    params_dict: ParamDict

    def setup(self):
        self.codec = self.codec_in.clone()

        self.starting_embedding = self.param(
            "starting_embedding", nn.zeros, (self.codec.embed_dim,)
        )

        # TODO: add params for all the unitialized models in model_dict

        self.trained_params_dict = self.param(
            "trained_params_dict",
            init_shared_trained_param_dict,
            self.model_dict,
            self.params_dict,
        )

    def __call__(self, x: Observation):
        """Compute the negative log-likelihood of sampling ``x`` from the model.

        We want to find the likelihood of sampling ``x`` starting from the ``starting_embedding``.
        Sampling is autoregressive for complex structures, so the probability of ``x`` is the product of the probabilities of
        sampling each ``x_i`` conditioned on having sampled ``x_1, ..., x_(i-1)``. Therefore we need the intermediate embeddings
        ``embed(x1, ..., x_(i-1))`` to compute the probability of sampling each ``x_i``.

        :param x: An observation.
        :return: The negative log-likelihood of ``x`` in the distribution generated by the model.
        """
        params_dict = self.params_dict.copy(add_or_replace=self.trained_params_dict)

        shared_dicts = self.model_dict, params_dict

        # Hence the computation is done in two steps:
        # 1/ compute the embeddings of the substructures of x. They are returned in the `context` variable:
        embedding, context = self.codec.encode(x=x, shared_dicts=shared_dicts)

        # 2/ predict the next column starting from the `starting_embedding` and as-if `x` was autoregressively sampled:
        prediction = self.codec.decode(
            conditioning_vector=self.starting_embedding,
            context=context,
            shared_dicts=shared_dicts,
        )

        # then we can evaluate P(x) = pred1(x1) x pred2(x2) x ...
        loss_x = self.codec.loss(x=x, prediction=prediction, shared_dicts=shared_dicts)
        return loss_x

    def sample(self):
        """:return: A sample from the distribution generated by the model."""
        params_dict = self.params_dict.copy(add_or_replace=self.trained_params_dict)
        shared_dicts = self.model_dict, params_dict

        sample = self.codec.sample(
            conditioning_vector=self.starting_embedding, shared_dicts=shared_dicts
        )
        return sample

    def example(self):
        """:return: An example observation of the data type expected by the model."""
        shared_dicts = self.model_dict, {}
        return self.codec_in.example(shared_dicts)


class MetaLearner:
    """The MetaLearner is the complete generative model. The type of the data it generates is defined by the
    provided ``Codec``.

    :param codec_in: The codec corresponding to the data type generated by the model.
    :param model_dict: A collection of codecs that can be shared in the type-tree
    :param params_dict: A collection of parameters for the shared codecs. For each model in ``model_dict``:

        - If parameters are provided, they are saved and used as-is, without being trained. It is still possible to finetune them using the :class:`gallimimus.codec.LoraCodec`
        - If the parameters are not provided, they are initialized as parameters of the MetaLearner, and then trained."""

    def __init__(self, codec_in: Codec, model_dict: Dict[str, Codec], params_dict: Dict[str, VariableDict]):
        # the MetaLearner is created by vmapping the methods of the UnitMetaLearner defined above.
        # `unit_metalearner` is a stateful flax module, we convert all the methods we need to pure jax functions before vmapping:

        model_dict = flax.core.frozen_dict.freeze(model_dict)
        params_dict = flax.core.frozen_dict.freeze(params_dict)
        unit_metalearner = UnitMetaLearner(
            codec_in=codec_in,
            model_dict=model_dict,
            params_dict=params_dict,
            parent=None,
        )

        self.init_fun = lambda rng, x: unit_metalearner.init(
            rngs={"params": rng},
            x=x,
        )

        self.apply_fun = lambda params, x: unit_metalearner.apply(
            variables={"params": params},
            x=x,
        )

        self.sample_fun = lambda params, rng: unit_metalearner.apply(
            variables={"params": params},
            rngs={"sample": rng},
            method="sample",
        )

        self.example = unit_metalearner.example()

    def init(self, rng: PRNGKeyArray) -> VariableDict:
        """Initalize the parameters of the model.

        :param rng: A Jax PRNGKey.
        :return: A Pytree of initial parameters."""
        # x is a single observation (and not a batch), the example provided by the Codecs can be used
        params = self.init_fun(rng=rng, x=self.example)["params"]
        return params

    def sample(self, params: VariableDict, rng: PRNGKeyArray, size: int):
        """Sample a batch of chosen size from the distribution represented by the model with the provided parameters.

        :param params: A Pytree of parameters.
        :param rng: A Jax PRNGKey.
        :param size: The number of requested samples.
        :return: A batch of samples of chosen size, stacked on the first dimension of the leaves.
        """
        vmapped_sample_fun = jax.vmap(self.sample_fun, in_axes=(None, 0))

        rngs = jax.random.split(rng, size)
        samples, embeddings = vmapped_sample_fun(params, rngs)
        return samples

    def batch_loss(
        self, params: VariableDict, xs: Observation
    ) -> Tuple[jnp.ndarray, Any]:
        """For a batch of observations ``xs``, compute the average loss (the average NLL of ``x`` in the distribution represented
        by ``params``) and its associated gradient.

        :param params: A Pytree of parameters.
        :param xs: A batch of observations, stacked on the first dimension of the leaves.
        :return: A tuple containing the average loss of the batch (an array of shape ``()``) and its gradient with respect to the parameters
            (a Pytree of the same shape as ``params``)."""
        vmapped_apply_fun = jax.vmap(self.apply_fun, in_axes=(None, 0))
        scalar_apply_fun = lambda params, xs: vmapped_apply_fun(params, xs).mean()
        return scalar_apply_fun(params, xs)

    def loss_and_grad(
        self, params: VariableDict, xs: Observation
    ) -> Tuple[jnp.ndarray, Any]:
        """For a batch of observations ``xs``, compute the average loss (the average NLL of ``x`` in the distribution represented
        by ``params``) and its associated gradient.

        :param params: A Pytree of parameters.
        :param xs: A batch of observations, stacked on the first dimension of the leaves.
        :return: A tuple containing the average loss of the batch (an array of shape ``()``) and its gradient with respect to the parameters
            (a Pytree of the same shape as ``params``)."""
        # vmap first, average the losses, then grad to obtain batch-level gradient
        grad_vmapped_apply_fun = jax.value_and_grad(fun=self.batch_loss)
        batch_loss, batch_grad = grad_vmapped_apply_fun(params, xs)
        return batch_loss, batch_grad

    def loss_and_per_example_grad(
        self, params: VariableDict, xs: Observation
    ) -> Tuple[jnp.ndarray, Any]:
        """For a batch of observations ``xs``, compute the average loss and *per-instance* gradients
        (per-instance gradients are required for DP-SGD, so that they can be clipped).

        :param params: A Pytree of parameters.
        :param xs: A batch of observations, stacked on the first dimension of the leaves.
        :return: A tuple containing the average loss of the batch (an array of shape ``()``) and the *per-instance* gradient with respect to the parameters
            (a stack of Pytrees of the same shape as ``params``, stacked on the first dimension of the leaves).
        """
        # grad then vmap to obtain instance-level gradients
        grad_apply_fun = jax.value_and_grad(fun=self.apply_fun)
        vmapped_grad_apply_fun = jax.vmap(grad_apply_fun, in_axes=(None, 0))

        per_ex_loss, per_ex_grad = vmapped_grad_apply_fun(params, xs)
        return per_ex_loss.mean(), per_ex_grad
