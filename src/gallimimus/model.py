"""
A genererative model for complex and hierarchical data
"""
from __future__ import annotations

import jax
from flax import linen as nn

from gallimimus.codec.abstract_codec import Codec, Observation

from flax.core.scope import VariableDict


class UnitMetaLearner(nn.Module):
    """The ``codec`` is embedded in a ``Metalearner`` which holds an additionnal ``starting_embedding`` parameter.
    The ``starting_embedding`` is the conditioning vector from which the autoregressive sampling starts.

    This module works with single observations (not batches). In practice, the model is vmapped over in the following ``BatchMetaLearner``.
    """

    codec_in: Codec

    def setup(self):
        """:meta private:"""
        self.codec = self.codec_in.clone()

        self.starting_embedding = self.param(
            "starting_embedding", nn.zeros, (self.codec.embed_dim,)
        )

    def __call__(self, x: Observation):
        """Compute the negative log-likelyhood of sampling ``x`` from the model.

        We want to find the likelyhood of sampling ``x`` starting from the ``starting_embedding``.
        Sampling is autoregressive for complex structures, so the probability of ``x`` is the product of the probabilities of
        sampling each ``x_i`` conditioned on having sampled ``x_1, ..., x_(i-1)``. Therefore we need the intermediate embeddings
        ``embed(x1, ..., x_(i-1))`` to compute the probability of sampling each ``x_i``.

        :param x: An observation.
        :return: The negative log-likelyhood of ``x`` in the distribution generated by the model.
        """

        # Hence the computation is done in two steps:
        # 1/ compute the embeddings of the substructures of x. They are returned in the `context` variable:
        embedding, context = self.codec.encode(x=x)

        # 2/ predict the next column starting from the `starting_embedding` and as-if `x` was autoregressively sampled:
        prediction = self.codec.decode(
            conditioning_vector=self.starting_embedding, context=context
        )

        # then we can evaluate P(x) = pred1(x1) x pred2(x2) x ...
        loss_x = self.codec.loss(x=x, prediction=prediction)
        return loss_x

    def sample(self):
        """:return: A sample from the distribution generated by the model."""
        sample = self.codec.sample(conditioning_vector=self.starting_embedding)
        return sample

    def example(self):
        """:return: An example observation of the data type expected by the model."""
        return self.codec_in.example()


class MetaLearner:
    """The MetaLearner is the complete generative model. The type of the data it generates is parametrized by the
    provided ``Codec``.

    :param codec_in: The codec corresponding to the data type generated by the model."""

    def __init__(self, codec_in: Codec):
        # the MetaLearner is created by vmapping the methods of the UnitMetaLearner defined above.
        # `unit_metalearner` is a stateful flax module, we convert all the methods we need to pure jax functions before vmapping:
        unit_metalearner = UnitMetaLearner(codec_in=codec_in, parent=None)

        self.init_fun = lambda rng, x: unit_metalearner.init(
            rngs={"params": rng},
            x=x,
        )

        self.apply_fun = lambda params, x: unit_metalearner.apply(
            variables={"params": params},
            x=x,
        )

        self.sample_fun = lambda params, rng: unit_metalearner.apply(
            variables={"params": params},
            rngs={"sample": rng},
            method="sample",
        )

        self.example = unit_metalearner.example()

    def init(self, rng) -> VariableDict:
        """Initalize the parameters of the model.

        :param rng: A Jax PRNGKey.
        :return: A Pytree of initial parameters."""
        # x is a single observation (and not a batch), the example provided by the Codecs can be used
        params = self.init_fun(rng=rng, x=self.example)["params"]
        return params

    def loss_and_per_example_grad(self, params: VariableDict, xs: Observation):
        """For a batch of observations ``xs``, compute the average loss and *per-instance* gradients
        (per-instance gradients are required for DP-SGD, so that they can be clipped).
        """
        # grad then vmap to obtain instance-level gradients
        grad_apply_fun = jax.value_and_grad(fun=self.apply_fun)
        vmapped_grad_apply_fun = jax.vmap(grad_apply_fun, in_axes=(None, 0))

        per_ex_loss, per_ex_grad = vmapped_grad_apply_fun(params, xs)
        return per_ex_loss.mean(), per_ex_grad

    def loss_and_grad(self, params: VariableDict, xs: Observation):
        """For a batch of observations ``xs``, compute the average loss and its associated gradient."""
        # vmap first, average the losses, then grad to obtain batch-level gradient
        vmapped_apply_fun = jax.vmap(self.apply_fun, in_axes=(None, 0))
        scalar_apply_fun = lambda params, xs: vmapped_apply_fun(params, xs).mean()
        grad_vmapped_apply_fun = jax.value_and_grad(fun=scalar_apply_fun)

        batch_loss, batch_grad = grad_vmapped_apply_fun(params, xs)
        return batch_loss, batch_grad

    def sample(self, params: VariableDict, rng, size: int):
        """Sample a batch of chosen size from the model.

        :param params:
        :param rng:
        :param size:
        :return: A batch of chosen size from the model."""
        vmapped_sample_fun = jax.vmap(self.sample_fun, in_axes=(None, 0))

        rngs = jax.random.split(rng, size)
        samples, embeddings = vmapped_sample_fun(params, rngs)
        return samples
